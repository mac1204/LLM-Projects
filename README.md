# LLM-Projects

Welcome to the repository that showcases various aspects of working with Large Language Models (LLMs) such as **Mistral**, **LLaMA**, and **Phi-2**, focusing on advanced tasks like **inference**, **Retrieval-Augmented Generation (RAG)**, **fine-tuning**, and more cutting-edge techniques like **Knowledge Transfer Optimization (KTO)** and **Direct Preference Optimization (DPO)**.

This repository contains Python notebooks that demonstrate the following tasks:

## Contents

- **LLM Inference**
  - Perform inference with leading LLMs such as Mistral, LLaMA, and Phi-2.
  - Explore different generation strategies, model outputs, and practical use cases.
  
- **Retrieval-Augmented Generation (RAG)**
  - Implement and understand how RAG combines external knowledge sources with LLMs for enhanced responses.
  - Examples of integrating external document stores for efficient retrieval with Mistral, LLaMA, etc.

- **Model Fine-Tuning**
  - Demonstrations of tuning LLMs on custom datasets to adapt to specific tasks.
  - Fine-tuning techniques across different LLM architectures.

- **Knowledge Transfer Optimization (KTO)**
  - Examples of using KTO for transferring knowledge between models effectively.
  - Practical implementations and results of optimized transfer learning techniques.

- **Direct Preference Optimization (DPO)**
  - Understand and implement preference-based learning using DPO.
  - Techniques for improving model performance based on human or system preferences.

## Models Covered

- **Mistral**: Demonstrating inference and fine-tuning with this powerful open-source LLM.
- **LLaMA**: A versatile, fine-tunable model used for inference and downstream tasks.
- **Phi-2**: Cutting-edge model for various generative tasks, with focus on fine-tuning and optimization.

## Notebooks

The repository is organized into several Jupyter Notebooks, each focusing on specific tasks. You can explore these notebooks to understand how to work with different LLMs and techniques without the hassle of installation or setting up complex environments.

- `inference_mistral_llama_phi2.ipynb`: Run LLM inference using Mistral, LLaMA, and Phi-2 models.
- `rag_mistral_llama_phi2.ipynb`: Implement Retrieval-Augmented Generation with these LLMs.
- `tuning_mistral_llama_phi2.ipynb`: Fine-tuning examples with specific datasets for Mistral, LLaMA, and Phi-2.
- `kto_examples.ipynb`: Knowledge Transfer Optimization techniques in action.
- `dpo_examples.ipynb`: Direct Preference Optimization for performance enhancement.

## How to Use

These notebooks are pre-configured to work in most standard Jupyter environments. Simply open and run the cells to explore each concept. Each notebook is thoroughly commented to guide you through the implementation steps, offering insights into how the models function and how to extend the examples to fit your own use cases.

---

Feel free to explore the repository, use the notebooks for your projects, and contribute if you have improvements or additional techniques to share!
